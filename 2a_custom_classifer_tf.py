"""This code demonstrates how to use BERT for a custom classification
model in TensorFlow.

TensorFlow official documentation goes over this same dataset, which can
be found here: https://www.tensorflow.org/text/tutorials/classify_text_with_bert.
They do not use the Transformers package, and I do not recommend using it.
But it's helpful to understand what we're doing.


It's possible to do most of this *within* Transformers only using the
`BertForSequenceClassification`, but I think this demonstrates how
BERT works better.
"""

#################################
# Example 2: Classification Model
#################################

# Stacking BERT on top of your own model
import os
import tensorflow as tf
from time import time
from tqdm import tqdm
from transformers import (
    BertTokenizer,
    TFBertModel,
)
from datasets import load_dataset
import numpy as np

# USER SPECIFICATIONS
# BERT_MODEL_NAME = "google/bert_uncased_L-4_H-256_A-4"
BERT_MODEL_NAME = r"C:\Users\jgolliher\OneDrive - American Bureau of Shipping\BERT Models\bert_uncased_L-4_H-256_A-4"  # Location of BERT model
MODEL_SAVE_LOC = "C:/Users/jgolliher"  # Where to save model during training
BATCH_SIZE = 72
NUM_EPOCHS = 5
PATIENCE = 2

############### AUTOGENERATED
print(f"BERT MODEL: {BERT_MODEL_NAME}")
print(f"BATCH_SIZE: {BATCH_SIZE}")
print(f"NUM_EPOCHS: {NUM_EPOCHS}")
print(f"PATIENCE: {PATIENCE}")

if BERT_MODEL_NAME == "":
    raise ValueError(f"BERT_MODEL_NAME was not specified!")
if not os.path.exists(BERT_MODEL_NAME):
    raise ValueError(f"BERT_MODEL_NAME is not a valid path!")
if MODEL_SAVE_LOC == "":
    raise ValueError(f"SAVE_LOC was not specified!")
if not os.path.exists(MODEL_SAVE_LOC):
    raise ValueError(f"MODEL_SAVE_LOC is not a valid path!")

# Load tokenizer
tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)

###################
# Load IMDB Dataset
###################

imdb = load_dataset("imdb")


def preprocess_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=512,
        return_attention_mask=True,
        return_token_type_ids=False,
    )


tokenized_imdb = imdb.map(preprocess_function, batched=True)

# Pull out input_ids, attention_masks, and labels

# Train: Used to fit model
x_train_input_ids = np.array(tokenized_imdb["train"]["input_ids"])
x_train_attention_mask = np.array(tokenized_imdb["train"]["attention_mask"])
y_train = np.array(tokenized_imdb["train"]["label"])

# Val: Used to test performance after each epoch (to avoid overfitting)
x_val_input_ids = np.array(tokenized_imdb["test"]["input_ids"])
x_val_attention_mask = np.array(tokenized_imdb["test"]["attention_mask"])
y_val = np.array(tokenized_imdb["test"]["label"])

# Test: Holdout set.
x_test_input_ids = np.array(tokenized_imdb["unsupervised"]["input_ids"])
x_test_attention_mask = np.array(tokenized_imdb["unsupervised"]["attention_mask"])
y_test = np.array(tokenized_imdb["unsupervised"]["label"])

############### AUTOGENERATED
# Calculate steps_per_epoch
steps_per_epoch = len(y_train) // BATCH_SIZE
if len(y_train) % BATCH_SIZE != 0:
    steps_per_epoch += 1

print(f"steps_per_epoch: {steps_per_epoch}")

####################
# TensorFlow Datasets
####################

"""Build TensorFlow datasets

Most useful for GPU training, but creates a clean object
to pass to the model. Also enables batching, shuffling, etc.
"""

train_dataset = tf.data.Dataset.from_tensor_slices(
    (
        {"input_ids": x_train_input_ids, "attention_mask": x_train_attention_mask},
        y_train,
    )
)

# Fits model
train_dataset = train_dataset.shuffle(buffer_size=len(y_train))
train_dataset = train_dataset.batch(BATCH_SIZE)
train_dataset = train_dataset.cache()
train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)

# Validates during training (we don't want to shuffle it)
val_dataset = tf.data.Dataset.from_tensor_slices(
    ({"input_ids": x_val_input_ids, "attention_mask": x_val_attention_mask}, y_val)
)
val_dataset = val_dataset.batch(BATCH_SIZE)
val_dataset = val_dataset.cache()
val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)

# Holdout set
test_dataset = tf.data.Dataset.from_tensor_slices(
    ({"input_ids": x_test_input_ids, "attention_mask": x_test_attention_mask}, y_test)
)


##################
# TensorFlow Model
##################

"""
It's best practice to specify the input and output dtypes because
it's common on GPU to enable mixed precision. This changes some layers
to 16-bit precision, but input and output layers should be in 32-bit
precision.

To enable mixed-precision:
>>> print("Setting mixed-precision")
>>> policy = tf.keras.mixed_precision.Policy("mixed_float16")
>>> tf.keras.mixed_precision.set_global_policy(policy)
>>> print("Compute dtype: %s" % policy.compute_dtype)
>>> print("Variable dtype: %s" % policy.variable_dtype)
"""

# Input layers
input_ids = tf.keras.layers.Input(shape=(512,), name="input_ids", dtype=tf.int32)
attention_mask = tf.keras.layers.Input(
    shape=(512,), name="attention_mask", dtype=tf.int32
)


# Define BERT model layer, best practice is to create custom layer
class BertLayer(tf.keras.layers.Layer):
    def __init__(self, bert_model_name, trainable=False):
        super(BertLayer, self).__init__()
        self.bert = TFBertModel.from_pretrained(bert_model_name)
        self.bert.trainable = trainable

    def call(self, inputs):
        input_ids, attention_mask = inputs
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        return outputs["pooler_output"]


# Create bert_layer and bert_output
bert_layer = BertLayer(BERT_MODEL_NAME, trainable=False)
bert_output = bert_layer([input_ids, attention_mask])


# Define dense and output layer
dense = tf.keras.layers.Dense(units=256, activation="relu", name="dense")(bert_output)
output = tf.keras.layers.Dense(
    units=1, activation="sigmoid", name="output", dtype=tf.float32
)(dense)

# Create model
model = tf.keras.Model(
    inputs={"input_ids": input_ids, "attention_mask": attention_mask}, outputs=output
)

# Model summary
model.summary()


#########################
# Learning Rate Scheduler
#########################

"""
BERT was trained with a linear decay learning rate with a warm-up
phase over the first 10% of training steps (num_warmump_steps).
We'll create a custom class to do this.

You could ignore all of this and set a fixed learning rate
while compiling the model.
"""


@tf.keras.saving.register_keras_serializable(package="custom")
class WarmupLinearDecayScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):
    """
    Custom learning rate scheduler with warmup and linear decay.

    Args:
        initial_learning_rate (float): The initial learning rate.
        warmup_steps (int): Number of steps for the warmup phase.
        total_steps (int): Total number of training steps.
        end_learning_rate (float): Final learning rate after decay. Default is 0.0.
    """

    def __init__(
        self, initial_learning_rate, warmup_steps, total_steps, end_learning_rate=0.0
    ):
        super(WarmupLinearDecayScheduler, self).__init__()
        if warmup_steps < 0 or total_steps <= 0 or warmup_steps >= total_steps:
            raise ValueError("`warmup_steps` must be >= 0 and < `total_steps`.")
        if initial_learning_rate <= 0:
            raise ValueError("`initial_learning_rate` must be > 0.")
        self.initial_learning_rate = initial_learning_rate
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.end_learning_rate = end_learning_rate

    def __call__(self, step):
        step = tf.cast(step, tf.float32)
        epsilon = 1e-8  # Small constant to avoid division by zero
        return tf.cond(
            step < self.warmup_steps,
            lambda: self.initial_learning_rate * (step / self.warmup_steps),
            lambda: self.end_learning_rate
            + (self.initial_learning_rate - self.end_learning_rate)
            * (
                1.0
                - (step - self.warmup_steps)
                / (self.total_steps - self.warmup_steps + epsilon)
            ),
        )

    def get_config(self):
        return {
            "initial_learning_rate": self.initial_learning_rate,
            "warmup_steps": self.warmup_steps,
            "total_steps": self.total_steps,
            "end_learning_rate": self.end_learning_rate,
        }


# Define learning rate schedule object
lr_schedule = WarmupLinearDecayScheduler(
    initial_learning_rate=3e-5,
    warmup_steps=int(0.1 * steps_per_epoch * NUM_EPOCHS),
    total_steps=steps_per_epoch * NUM_EPOCHS,
    end_learning_rate=0.0,
)

##################
# Callbacks
##################

"""
TensorFlow callbacks are utilities that allow you to monitor, modify,
and extend the behavior of a model during training, evaluation, or inference.
They can be used for tasks like early stopping, saving checkpoints,
adjusting learning rates, or logging metrics. Callbacks enhance training
efficiency and help automate repetitive tasks.
^^ Thank you ChadGPT

Quick note on early stopping:
Setting `restore_best_weights=True` means that after training, TensorFlow
will reload the best weights into the environmnet. This differs from the
PyTorch example, which *does not* do this.
"""

# Save model checkpoints
checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(
    filepath=MODEL_SAVE_LOC,
    save_freq="epoch",
    save_best_only=True,
    save_weights_only=True,
)

# Early stopping (stop training when we start overfitting)
early_stopping_cb = tf.keras.callbacks.EarlyStopping(
    patience=PATIENCE,
    restore_best_weights=True,  # Will reload best weights after training finishes
    monitor="val_loss",
)


# Custom callback to save the time, in seconds, each epoch took
class TimeHistory(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.times = []

    def on_epoch_begin(self, epoch, logs={}):
        self.epoch_time_start = time()

    def on_epoch_end(self, epoch, logs={}):
        self.times.append(time() - self.epoch_time_start())


# Define custom callback object
time_callback = TimeHistory()

###################
# Compile and Train
###################

model.compile(
    optimizer=tf.keras.optimizers.AdamW(learning_rate=lr_schedule),
    loss="binary_crossentropy",
    metrics=["accuracy"],
)

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=NUM_EPOCHS,
    steps_per_epoch=steps_per_epoch,
    callbacks=[checkpoint_cb, early_stopping_cb, time_callback],
)
